id: 4.1
epic: 4
title: Metrics Wall (latency, tokens, %LLM, explainability)
status: draft
story: |
  As an administrator, I want to view a dashboard of key metrics for the application so that I can monitor its performance, costs, and reliability.
acceptance_criteria:
  - Capture and display p95 latency, tokens_per_doc, %docs_invoking_LLM
  - Explainability rate (findings with snippet+rule id)
  - Admin‑only page
notes:
  - Backend service to collect and expose metrics data.
  - Frontend page (likely admin-only) to display metrics, possibly using charts.
  - Integration with the token ledger (Story 2.4) for token metrics.
  - Define how latency is measured (e.g., from job queue to completion).
tasks: []
dev_agent_record: {}

### dev_spec

- Backend: `services/metrics.py` collects p95 job latency (enqueue→done), tokens_per_doc avg (from ledger), %docs invoking LLM, explainability rate (findings with snippet+rule id / total).
- API: `GET /api/admin/metrics` returns metrics JSON; admin-only.
- UI: `/admin/metrics` displays stats and/or charts.

### qa_tests

- Metric math correctness (p95, averages, ratios); authZ enforced.
- UI renders values matching API; updates with simulated activity.
