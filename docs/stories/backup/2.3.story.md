id: 2.3
epic: 2
title: Weak-Language Lexicon v0
status: Draft
story: |
  As a detection system, I need to apply a lexicon of weak language terms to findings to potentially downgrade a 'Pass' verdict to 'Weak' if no strong counter-anchors are found.
acceptance_criteria:
  - Apply lexicon inside evidence window
  - Downgrade Passâ†’Weak unless counter-anchor present
  - Configurable lexicon file

dev_notes:
  Previous Story Insights:
  - The detector runner (`2.2-detector-runner.md`) is now in place and produces initial verdicts. This story will add a post-processing step to that. [Source: docs/stories/2.2-detector-runner.md]

  Data Models:
  - The `Finding` Pydantic model will be used. No changes to the schema are expected. [Source: docs/architecture/5-data-model-pydantic.md]
  ```python
  class Finding(BaseModel):
      detector_id: str
      rule_id: str
      verdict: Literal["pass","weak","missing","needs_review"]
      snippet: str
      page: int
      start: int
      end: int
      rationale: str
      reviewed: bool = False
  ```

  API Specifications:
  - No new API endpoints required.
  - Existing `/api/analyses/{id}/findings` endpoint will automatically reflect downgraded verdicts.
  - The `/api/rules/summary` endpoint may need updating to reflect lexicon usage.
  [Source: docs/architecture/6-api-contracts-mvp-frozen.md]

  Component Specifications:
  - New service: `apps/api/blackletter_api/services/weak_lexicon.py` containing the `apply_downgrade` function
  - Modified service: `apps/api/blackletter_api/services/detector_runner.py` to integrate the downgrade logic
  - Modified service: `apps/api/blackletter_api/services/rulepack_loader.py` to load the weak language lexicon
  - Configuration: `core-config.yaml` to include the `WEAK_LEXICON_ENABLED` toggle
  - Lexicon file: `apps/api/blackletter_api/rules/lexicons/weak_language.yaml` with terms and counter-anchors
  [Source: docs/architecture/4-service-decomposition.md]

  File Locations:
  - New logic should be implemented in `apps/api/blackletter_api/services/weak_lexicon.py`.
  - The lexicon file should be located at `apps/api/blackletter_api/rules/lexicons/weak_language.yaml`.
  - The `detector_runner.py` service will be modified to incorporate this new logic.
  [Source: docs/architecture/source_tree.md]

  Testing Requirements:
  - Unit tests should cover:
      - Lexicon matches causing a downgrade.
      - Counter-anchor presence preventing a downgrade.
      - Case-insensitive and whole-word boundary matching.
      - The feature toggle disabling the functionality.
      - Edge cases:
          - Empty findings list
          - Missing lexicon file (should handle gracefully)
          - Malformed lexicon file (should handle gracefully)
          - Overlapping weak terms and counter-anchors
          - Very long evidence windows
          - Unicode characters in text
          - Performance with large numbers of findings
  [Source: docs/architecture/10-quality-gates-testing.md]
  - Integration tests should verify the end-to-end flow from detection to findings with downgrades. [Source: docs/architecture/10-quality-gates-testing.md]

  Technical Constraints:
  - The implementation must adhere to the existing tech stack (Python 3.11, FastAPI, Pydantic). [Source: docs/architecture/tech_stack.md]
  - Code should be formatted with `black` and linted with `ruff`. [Source: docs/architecture/coding_standards.md]
  - Regular expressions should use word boundaries (`\b`) for precise matching
  - Case normalization should use `.lower()` for consistent comparison
  - Thread safety is not required as processing is single-threaded per analysis
  - Memory usage should be minimized when processing large documents

tasks:
  - [ ] Backend: Implement Lexicon Loading (AC: 3) - In `rulepack_loader.py`, add logic to load `rules/lexicons/weak_language.yaml`. The lexicon should be made available to the `detector_runner` service via the rulepack. Handle errors gracefully if the lexicon file is missing or malformed. (Source: docs/architecture/4-service-decomposition.md)
  - [ ] Backend: Implement Downgrade Logic (AC: 1, 2) - Create a new service `weak_lexicon.py`. Implement a function `apply_downgrade(findings, windows, counter_anchors)` that post-processes findings. This function should downgrade 'Pass' to 'Weak' based on the lexicon terms and counter-anchors. Use regex with word boundaries (`\b`) for precise matching. Apply case normalization with `.lower()` for consistent comparison. Integrate this function into `detector_runner.py` after initial verdict generation. (Source: docs/architecture/4-service-decomposition.md)
  - [ ] Config: Add Feature Toggle - Add a feature toggle `WEAK_LEXICON_ENABLED` to `core-config.yaml` with default value "1". The downgrade logic should only run if this is enabled. Short-circuit early if the feature is disabled to avoid unnecessary processing. (Source: docs/architecture/4-service-decomposition.md)
  - [ ] Tests: Unit Tests (AC: 1, 2, 3) - In `tests/unit/`, add `test_weak_lexicon.py`. Add tests for the downgrade logic, counter-anchor overrides, and word boundaries. Add tests for edge cases (empty findings, missing lexicon, malformed lexicon). Add a test to ensure the feature toggle is respected. Add performance tests to ensure processing time is reasonable. (Source: docs/architecture/10-quality-gates-testing.md)
  - [ ] Integration Tests - Create integration tests that verify the end-to-end flow from detection to findings with downgrades. Test with both enabled and disabled feature toggle. Verify that persisted findings reflect correct verdicts. (Source: docs/architecture/10-quality-gates-testing.md)

change_log:
  - 2025-08-30: Story drafted by Scrum Master (Bob).